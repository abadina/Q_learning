{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#class used to represent the 1d grid.\n",
    "class grid_1d:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dim = 6\n",
    "        self.pos_goal = 5 # the goal position\n",
    "        self.reward_goal = 1\n",
    "        self.pos_trap = 0 # the trap position\n",
    "        self.reward_trap = -1\n",
    "        self.start = 2 # the starting position\n",
    "        self.s = self.start #the actual position\n",
    "        self.complete = False #True if we reached a terminal state - terminal state = (goal position, trap position)\n",
    "        self.possible_actions = [0, 1] # 0 for left 1 for right\n",
    "    \n",
    "    #to display the grid\n",
    "    def display_grid(self):\n",
    "        print(\"-\" * (self.dim * 5 + 5))\n",
    "        row = []\n",
    "        for i in range(self.dim):\n",
    "            if i == self.pos_goal :\n",
    "                row.append(\"|  G \")    \n",
    "            elif i == self.pos_trap :\n",
    "                row.append(\"|  T \")\n",
    "            elif i == self.start :\n",
    "                row.append(\"|  S \")\n",
    "            else:\n",
    "                row.append(\"|    \")\n",
    "        row.append(\"|\")\n",
    "        print(' '.join(row))    \n",
    "        print(\"-\" * (self.dim * 5 + 5))\n",
    "        \n",
    "\n",
    "    def step(self, a): # a is the action\n",
    "        \n",
    "        #when agent reaches a terminal state self.complete becomes True \n",
    "        if self.s == self.pos_goal:\n",
    "            self.complete = True\n",
    "            reward = self.reward_goal\n",
    "        elif self.s == self.pos_trap:\n",
    "            self.complete = True\n",
    "            reward = self.reward_trap\n",
    "        else:  \n",
    "            # move to left\n",
    "            if a == 0 and self.s > 0:\n",
    "                self.s -= 1\n",
    "            # move to right\n",
    "            elif a == 1 and self.s < self.dim - 1:\n",
    "                self.s += 1\n",
    "            reward = 0\n",
    "        return self.s, reward, self.complete\n",
    " \n",
    "    # to restart grid\n",
    "    def restart(self):\n",
    "        self.s = self.start\n",
    "        self.complete = False\n",
    "        return self.s\n",
    "    \n",
    "    \n",
    "    # to get and display policy\n",
    "    def policy(self, q_table):\n",
    "        policy=[]\n",
    "    \n",
    "        q_max, q_max_action=max_qs(q_table)\n",
    "        \n",
    "        #print(q_max)\n",
    "        #print(q_max_action)\n",
    "        print(\"-\" * (self.dim * 5 + 5))\n",
    "        for action in q_max_action:\n",
    "            if action == 0:\n",
    "                policy.append(\"|  L \")\n",
    "            elif action == 1:\n",
    "                policy.append(\"|  R \")\n",
    "        policy.append(\"|\")\n",
    "        print(' '.join(policy))\n",
    "        print(\"-\" * (self.dim * 5 + 5))\n",
    "                   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_qs(q_table): #used to get max values of q and associated actions between left and right\n",
    "    q_max=[]\n",
    "    q_max_action=[]\n",
    "    for q in range(len(q_table)):\n",
    "        q_max.append(max(q_table[q][0],q_table[q][1]))\n",
    "        if(max(q_table[q][0],q_table[q][1]) == q_table[q][0]):\n",
    "            q_max_action.append(0)\n",
    "        else :\n",
    "            q_max_action.append(1)\n",
    "        \n",
    "    return q_max,q_max_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "|  T  |     |  S  |     |     |  G  |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "mygrid = grid_1d()\n",
    "mygrid.display_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters chosen \n",
    "nb_episodes = 1000\n",
    "gamma = 0.9\n",
    "eps = 0.5\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position: 0\n",
      "-4.880024006256031\n",
      "-4.85637585917002\n",
      "\n",
      "\n",
      "position: 1\n",
      "-4.122890334521776\n",
      "6.54330557689164\n",
      "\n",
      "\n",
      "position: 2\n",
      "5.886746077579065\n",
      "7.274907347657242\n",
      "\n",
      "\n",
      "position: 3\n",
      "6.544695173652013\n",
      "8.084037972270364\n",
      "\n",
      "\n",
      "position: 4\n",
      "7.272547516558673\n",
      "8.98320154745949\n",
      "\n",
      "\n",
      "position: 5\n",
      "9.97666641154442\n",
      "9.98291112886439\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize all the values of q_table at zero\n",
    "q_table = np.zeros((mygrid.dim, len(mygrid.possible_actions)))\n",
    "\n",
    "# iterate through episodes\n",
    "for episode in range(nb_episodes):\n",
    "    actual_step = mygrid.restart() #step 0\n",
    "    done = False\n",
    "    #episode terminate when terminal state reached\n",
    "    while done == False:\n",
    "        # exploration\n",
    "        if np.random.rand() < eps:\n",
    "            action = np.random.choice(mygrid.possible_actions)\n",
    "        else:\n",
    "            # exploitation\n",
    "            action = np.argmax(q_table[actual_step])\n",
    "\n",
    "        next_step, reward, done = mygrid.step(action)\n",
    "        \n",
    "        # update the q-table\n",
    "        q_table[actual_step][action] += alpha*(reward + gamma*np.max(q_table[next_step])- q_table[actual_step][action])\n",
    "        \n",
    "        actual_step = next_step\n",
    "            \n",
    "#display q_table\n",
    "for q in range(len(q_table)):\n",
    "    print(\"position: \"+str(q))\n",
    "    for p in range(len(q_table[0])):   \n",
    "        print(q_table[q][p])\n",
    "    print(\"\\n\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the closer we get to the goal the larger the values are .\n",
    "And the larger values of both (left and right) are the values that produce a policy.\n",
    "\n",
    "In our case we got all q_table[i][1] larger than the q_table[i][0] and that's how we know that in order to reach the goal we should move to the right and if we move to the left we will only get farther from the goal ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "|  R  |  R  |  R  |  R  |  R  |  R  |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "mygrid.policy(q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
